{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- Understand Linear Regression modelling and apply it to a real dataset \n",
    "- Explore and analyse a real dataset Bikeshare Dataset to predict rental rate \n",
    "- Implement an ordinary linear regression model on a dataset satisfying linearity assumption using scikit-learn library.\n",
    "- Understand and identify multicollinearity in a multiple regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Linear Regression\n",
    "A linear regression model predicts the value of a continuous target variable as a weighted sum of the input features. The target variable is denoted as $y$ and the input features are denoted as a feature vector $x$. It is assumed that $y$ is a linear function of one or more input features with some random noise.\n",
    "The relationship between each input features $x^{(1)}, x^{(2)}, ..., x^{(d)}$ and the target $y$ for one observation in the sample is:\n",
    "\\begin{equation}\n",
    "y=\\beta_{0}+\\beta_{1}x^{(1)}+\\ldots+\\beta_{d}x^{(p)}+\\epsilon\n",
    "\\end{equation}\n",
    "where $\\beta_{j}$ are the learned feature weights or coefficients, $x^{(j)}$ are the input features and $\\epsilon$ is the noise or error of the prediction. Each of the input features (e.g. $x^{(j)}$) has a highest degree of 1, therefore the equation represents a **linear** relationship between input and output target. A given set of weight values $\\beta_{j}$ gives a predicted output target which will then be compared against the actual label of that observation. A `loss` function $loss(\\hat{y}_i, y_i)$ is defined as the difference between the predicted target and the actual label for each training example $i$. It depends on which modeling problem we are trying to solve that we have different loss functions. In this linear regression problem we are going to use the Squared Error loss function which is the squared difference between predicted value and the actual label.\n",
    "\\begin{equation}\n",
    " loss = (\\hat{y}_{i} - y_{i})^{2}\n",
    "\\end{equation}\n",
    "The best weights or coefficients are the ones minimising the error or the difference between the predicted targets and the actual labels for all the training examples in the dataset (A side note: for now we use the training data as the ground for determining the best weights, we will revisit this defintion later on once we reach the definition of training error and test error). Therefore, we have to sum the loss over all training examples and calculate the Mean Squared Error (MSE) loss. This process of fitting the equation through all the data in the training examples is called training.\n",
    "\\begin{equation}\n",
    "\\hat{\\boldsymbol{\\beta}}=\\arg\\!\\min_{\\beta_0,\\ldots,\\beta_p}\\sum_{i=1}^n(\\hat{y}_{i}- y_{i})^{2}\n",
    "\\end{equation}\n",
    "\n",
    "The above equation uses $argmin$ as a function to determine the set of $\\beta$s that minimise the overall MSE.\n",
    "\n",
    "The equation with the learned coefficients describe a line of best fit that minimises the difference between each actual points and the prediction values in a 2D space as in the below illustration. In a n-dimensional feature space, the equation describes a hyperplane.\n",
    "\n",
    "<center><img src='./assets/estimation.png'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression simple 1-D example\n",
    "- This part illustrates a complete process on applying a machine learning model to a dataset. \n",
    "- Demonstrate the use of a simple linear regression model on a 1-dimensional toy dataset\n",
    "- Use common libraries such as numpy, pandas, seaborn and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off by collecting/ generating the data. Here we generate a random sample of 100 2-D points following a uniform distribution $\\mathcal{N}(0,1)$ where the x-y coordinates related linearly to each other. There is random noise (e.g $\\epsilon$) introduced into the data generation process to scatter the points out of an obvious straight line. This is also simulating the noise coming from our input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Machine Learning model in scikit-learn is quite straight-forward. We usually follow 2 processes: \n",
    "- instantiate an instance of the model, \n",
    "- then fit the model with the input-output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model finishes training, we use it to predict the target from an unseen data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients of the linear regression model can be accessed by the `coef_` and `intercept_` attributes of the model instance. In this example, since we only have 1-D data point, we can access $\\beta_0$ and $\\beta_1$ as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonlinear relationship between features and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above toy example, we hope you have an idea of what linear regression is and how to train a linear regression model on a dataset using scikit-learn library. Now we are going to see how to apply ML model in a real dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduce-the-bikeshare-dataset\"></a>\n",
    "## Introduce the Capital Bikeshare Data Set\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This dataset contains the hourly and daily count of rental bikes between years 2011 and 2012 in Capital bikeshare system with the corresponding weather and seasonal informat (http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset)\n",
    "- The goal is to predict how many bikes will be rented depending on the weather and the day.\n",
    "- Here are the list of features that we are going to use:\n",
    " * datetime - hourly date + timestamp  \n",
    " * season -  1 = spring, 2 = summer, 3 = fall, 4 = winter \n",
    " * holiday - whether the day is considered a holiday\n",
    " * workingday - whether the day is neither a weekend nor holiday\n",
    " * weather:\n",
    "        1. Clear, Few clouds, Partly cloudy, Partly cloudy \n",
    "        2. Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "        3. Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "        4. Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n",
    " * temp - temperature in Celsius\n",
    " * atemp - \"feels like\" temperature in Celsius\n",
    " * humidity - relative humidity\n",
    " * windspeed - wind speed\n",
    " * casual - number of non-registered user rentals initiated\n",
    " * registered - number of registered user rentals initiated\n",
    " * count - number of total rentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find a set of coefficients that best predict the total rentals. The features in our input are temperature, windspeed, holiday etc...\n",
    "An example would be something like this:\n",
    "\n",
    "$total\\_rentals = 20 + -2 \\cdot temp + -3 \\cdot windspeed\\ +\\ ...\\ +\\ 0.1 \\cdot registered$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's build a Linear Regression Model in sklearn to apply to the bikeshare dataset\n",
    "- Select the appropriate features to give to the model as inputs\n",
    "- Perform feature engineering to transform the data types into the right format to be useful for the model to learn\n",
    "- Compare train and test error. Eventhough we mentioned in the introduction that the best set of coefficients is the one that minimises the train MSE, test MSE is actually more important in evaluating the performance of an ML model. It shows you how your model is going to perform on new unseen data which is eventually what it needs to perform in reality. Therefore we are going to check on both the train error, as well as the test error for every modification steps and for new model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LinearRegression model from sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "## We need to define a train function to reduce the amount of typing\n",
    "def train_linear(features, label, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Ins: features, labels, test_size\n",
    "    Outs: None but print out train MSE, test MSE\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=test_size, random_state=42)\n",
    "    # Instantiate a linear regression model and fit it through our training data\n",
    "    linreg = LinearRegression()\n",
    "    linreg.fit(X_train, y_train)\n",
    "    # Make predictions on our training\n",
    "    training_preds = linreg.predict(X_train)\n",
    "    \n",
    "    # Calculate and print the MSE\n",
    "    print('Training RMSE:', np.sqrt(metrics.mean_squared_error(y_train, training_preds)))\n",
    "    # Now apply the trained model on our test data and calculate the test error\n",
    "    test_preds = linreg.predict(X_test)\n",
    "    print('Test RMSE:', np.sqrt(metrics.mean_squared_error(y_test, test_preds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For continuous variable prediction, accuracy is not a good metrics, since we can never have the predicted the value to match 100% with the target value (remember the noise?). Therefore, we need to look at the errors that the prediction has and try to reduce it.\n",
    "Looking at a single test MSE or train MSE is not very informative as they are relative errors which describes the difference between the predicted outputs and the actual outputs. We normally want to look at the trend of these errors as trying out different models or after certain modifications to the features, then select the model or the configuration that yields the lowest train and test error. More on this later. For now, there are two important things that we want you to have in mind:\n",
    "- Know how to create a train function to take in input features and create a model to predict the output labels\n",
    "- Have a train RMSE and test RMSE to be used as baseline\n",
    "\n",
    "Let's use this as a baseline and we are going to see how other models and feature engineering techniques that you are going to learn later perform against this naive linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Evaluation Metrics:\n",
    "\n",
    "With $n$ is the number of training examples, we have the following error metrics:\n",
    "\n",
    "**Mean absolute error (MAE)** is the mean of the absolute value of the errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n",
    "- Easy to understand\n",
    "- Does not highlight the effect of outliers\n",
    "\n",
    "**Mean squared error (MSE)** is the mean of the squared errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "- The squared part is used to make derivative easier to deal with in some optimization algorithms\n",
    "- Highlight the significance of outliers\n",
    "\n",
    "**Root mean squared error (RMSE)** is the square root of the mean of the squared errors:\n",
    "\n",
    "$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n",
    "- Give the benefit that the errors' unit is the same as of the targets\n",
    "- Still maintain the squared to highlight the significance of outliers\n",
    "\n",
    "In scikit-learn:\n",
    "```\n",
    ">>> print('MAE:', metrics.mean_absolute_error(true, pred))\n",
    ">>> print('MSE:', metrics.mean_squared_error(true, pred))\n",
    ">>> print('RMSE:', np.sqrt(metrics.mean_squared_error(true, pred)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null RMSE is the RMSE that could be achieved by always predicting the mean response value. It is a benchmark against which you may want to measure your regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a NumPy array with the same shape as y_test.\n",
    "`\n",
    "y_null = np.zeros_like(y_test, dtype=float)\n",
    "assert len(y_null)==len(y_test)\n",
    "`\n",
    "\n",
    "\n",
    "Fill the array with the mean value of y_test.\n",
    "`\n",
    "y_null.fill(y_test.mean())\n",
    "y_null\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see from the above results, the columns `season`,`holiday`,`workingday` and `weather` should be of `categorical` data type.But the current data type is `integer` for those columns. Let us transform the dataset in the following ways so we can make it more useful for the model to learn.\n",
    "\n",
    "- Extract more information from the current datetime by creating new columns `date`, `hour`, `weekDay`, `month` from `datetime` column.\n",
    "- Convert the datatype of `season`, `holiday`, `workingday` and `weather` to category.\n",
    "- Drop the datetime column as we already extracted useful features from it.\n",
    "- Drop the `atemp` column because it conveys the same information as the `temp` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical data are represented using integers and hence some ordinal relationships may be wrongly interpreted by the learning algorithms.\n",
    "For example if there are five categories 'A', 'B', 'C', 'D' and 'E' that do not have any ordinal meanings, interprete them using integers from 1 to 5 may accidentially introduce comparability. Therefore, one-hot encoding can represent each of these categories using 5 binary vectors,or to be useful to the ML model, 4 of them are going to be used as the 5th vector can be infered from the first four. This is to remove collinearity between input features. Pandas has an option to remove this redundant column.\n",
    "\n",
    "For example, the season column which contains 4 integers: 1 - Spring, 2 - Summer, 3 - Autumn and 4- Winter can be represented as 4 binary vectors:\n",
    "- 1: [1, 0, 0, 0]\n",
    "- 2: [0, 1, 0, 0]\n",
    "- 3: [0, 0, 1, 0]\n",
    "- 4: [0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros/ Cons of Linear Regression\n",
    "Advantages of linear regression:\n",
    "\n",
    "- Simple to explain.\n",
    "- Highly interpretable.\n",
    "- Model training and prediction are fast.\n",
    "\n",
    "Disadvantages of linear regression:\n",
    "\n",
    "- Presumes a linear relationship between the features and the response.\n",
    "- Performance is in general not competitive with other ML models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
